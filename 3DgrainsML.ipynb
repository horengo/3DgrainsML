{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77772b2f-d4dd-4e7e-9224-a23ea811433f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3D shape identification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea5264e-6ecb-4ba0-b860-dc4b04dd4ca3",
   "metadata": {},
   "source": [
    "### Extract the measures from the 3D mesh files\n",
    "Specify the path to the folder where the 3D files are stored (line 209) <br> Specify the csv file name to store the extracted measures (line 204). This file will be stored in the same folder the files are stored. <br> <br> Please, note that folder path is also included in the measures output csv file. This is to be able to recover the class at a latter stage, assuming that the models are divided by subfolders according to class within the main folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1328a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull, QhullError\n",
    "from sklearn.decomposition import PCA\n",
    "from numpy.fft import fft\n",
    "\n",
    "def load_stl(folder_path):\n",
    "    # Function to load all STL files and extract vertices (Binary STL only)\n",
    "    vertices_dict = {}\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '**/*.stl'), recursive=True):\n",
    "        print(f\"Loading {file_path}...\")\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = np.fromfile(file, dtype=np.uint8, count=-1, offset=84)\n",
    "            data = data.reshape((-1, 50))\n",
    "            data = data[:, :48]\n",
    "            data = data.reshape((-1, 4))[:, 1:]\n",
    "            vertices = data.reshape((-1, 3))\n",
    "            vertices_dict[file_path] = vertices\n",
    "    return vertices_dict\n",
    "\n",
    "def compute_cross_sectional_areas(vertices, axis_index, n_sections=9):  # Modify according to the n_sections desired\n",
    "    # Compute cross-sectional areas along a specified axis\n",
    "    min_val = np.min(vertices[:, axis_index])\n",
    "    max_val = np.max(vertices[:, axis_index])\n",
    "    step = (max_val - min_val) / n_sections\n",
    "    areas = []\n",
    "    for i in range(n_sections):\n",
    "        val = min_val + i * step\n",
    "        cross_section = vertices[np.abs(vertices[:, axis_index] - val) < step / 2]\n",
    "        if len(cross_section) > 2:\n",
    "            # Projecting vertices to a plane perpendicular to the chosen axis\n",
    "            proj_vertices = np.delete(cross_section, axis_index, axis=1)\n",
    "            try:\n",
    "                hull = ConvexHull(proj_vertices)\n",
    "                areas.append(hull.area)\n",
    "            except QhullError:\n",
    "                areas.append(0)\n",
    "        else:\n",
    "            areas.append(0)\n",
    "    return areas\n",
    "\n",
    "def perform_pca_and_get_dimensions(vertices):\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(vertices)\n",
    "    transformed = pca.transform(vertices)\n",
    "    min_vals = np.min(transformed, axis=0)\n",
    "    max_vals = np.max(transformed, axis=0)\n",
    "    dimensions = max_vals - min_vals\n",
    "    return dimensions, pca.explained_variance_ratio_, pca.components_\n",
    "\n",
    "def calculate_curvature(vertices):\n",
    "    curvatures = []\n",
    "    for i in range(0, len(vertices), 3):  # Assuming vertices are in sets of 3 (triangular facets)\n",
    "        p1, p2, p3 = vertices[i], vertices[i+1], vertices[i+2]\n",
    "        v1 = p2 - p1\n",
    "        v2 = p3 - p1\n",
    "\n",
    "        # Calculate norms\n",
    "        norm_v1 = np.linalg.norm(v1)\n",
    "        norm_v2 = np.linalg.norm(v2)\n",
    "\n",
    "        if norm_v1 > 1e-6 and norm_v2 > 1e-6:  # Check to avoid division by zero\n",
    "            dot_product = np.dot(v1, v2)\n",
    "            dot_product = np.clip(dot_product / (norm_v1 * norm_v2), -1.0, 1.0)  # Clipping to avoid invalid values for arccos\n",
    "            angle = np.arccos(dot_product)\n",
    "            curvatures.append(angle)\n",
    "    \n",
    "    if curvatures:\n",
    "        return np.mean(curvatures)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def calculate_fourier_descriptors(vertices, num_descriptors=5):\n",
    "    # Calculating for XY plane\n",
    "    vertices_2d_xy = vertices[:, :2]\n",
    "    fourier_result_xy = fft(vertices_2d_xy, axis=0)\n",
    "    descriptors_xy = np.abs(fourier_result_xy[:num_descriptors]).flatten()\n",
    "\n",
    "    # Calculating for XZ plane\n",
    "    vertices_2d_xz = vertices[:, [0, 2]]  # X and Z coordinates\n",
    "    fourier_result_xz = fft(vertices_2d_xz, axis=0)\n",
    "    descriptors_xz = np.abs(fourier_result_xz[:num_descriptors]).flatten()\n",
    "\n",
    "    # Calculating for YZ plane\n",
    "    vertices_2d_yz = vertices[:, 1:]  # Y and Z coordinates\n",
    "    fourier_result_yz = fft(vertices_2d_yz, axis=0)\n",
    "    descriptors_yz = np.abs(fourier_result_yz[:num_descriptors]).flatten()\n",
    "\n",
    "    # Combine descriptors from all planes\n",
    "    return np.concatenate([descriptors_xy, descriptors_xz, descriptors_yz])\n",
    "\n",
    "def calculate_fourier_descriptors_for_cross_section(cross_section, num_descriptors=5):\n",
    "    # Assuming cross_section is a 2D array of vertices for the cross section\n",
    "    fourier_result = fft(cross_section, axis=0)\n",
    "    return np.abs(fourier_result[:num_descriptors]).flatten()\n",
    "\n",
    "def compute_fourier_descriptors_cross_sections(vertices, axis_index, n_sections=9, num_descriptors=5):  # Modify according to the n_sections desired\n",
    "    # Similar steps as in compute_cross_sectional_areas but with Fourier descriptors\n",
    "    min_val = np.min(vertices[:, axis_index])\n",
    "    max_val = np.max(vertices[:, axis_index])\n",
    "    step = (max_val - min_val) / n_sections\n",
    "    descriptors = []\n",
    "    for i in range(n_sections):\n",
    "        val = min_val + i * step\n",
    "        cross_section = vertices[np.abs(vertices[:, axis_index] - val) < step / 2]\n",
    "        if len(cross_section) > 2:\n",
    "            # Projecting vertices to a plane perpendicular to the chosen axis\n",
    "            proj_vertices = np.delete(cross_section, axis_index, axis=1)\n",
    "            fourier_descriptors = calculate_fourier_descriptors_for_cross_section(proj_vertices, num_descriptors)\n",
    "            descriptors.extend(fourier_descriptors)\n",
    "        else:\n",
    "            descriptors.extend([0] * num_descriptors * 2)  # Assuming 2D Fourier Descriptors\n",
    "    return descriptors\n",
    "\n",
    "\n",
    "def calculate_sphericity(volume, surface_area):\n",
    "    return (np.pi ** (1 / 3) * (6 * volume) ** (2 / 3)) / surface_area\n",
    "\n",
    "def calculate_roundness(vertices):\n",
    "    hull = ConvexHull(vertices)\n",
    "    sphere_radius = np.cbrt((3 * hull.volume) / (4 * np.pi))\n",
    "    return hull.area / (4 * np.pi * sphere_radius ** 2)\n",
    "\n",
    "def calculate_compactness_aabb(vertices):\n",
    "    min_vals = np.min(vertices, axis=0)\n",
    "    max_vals = np.max(vertices, axis=0)\n",
    "    dimensions = max_vals - min_vals\n",
    "    hull = ConvexHull(vertices)\n",
    "    volume = hull.volume\n",
    "    return volume / (np.linalg.norm(dimensions) ** 3)\n",
    "\n",
    "def extract_features(vertices):\n",
    "    dimensions, variance_ratios, principal_axes = perform_pca_and_get_dimensions(vertices)\n",
    "    hull = ConvexHull(vertices)\n",
    "    volume = hull.volume\n",
    "    surface_area = hull.area\n",
    "    aspect_ratios = [dimensions[0]/dimensions[1], dimensions[1]/dimensions[2], dimensions[0]/dimensions[2]]\n",
    "    elongation = max(dimensions) / min(dimensions)\n",
    "    compactness = volume / (np.linalg.norm(dimensions) ** 3)\n",
    "    compactness_aabb = calculate_compactness_aabb(vertices)\n",
    "    eccentricity = np.sqrt(1 - (min(dimensions) / max(dimensions)) ** 2)\n",
    "    cross_sectional_areas_x = compute_cross_sectional_areas(vertices, axis_index=0)\n",
    "    cross_sectional_areas_y = compute_cross_sectional_areas(vertices, axis_index=1)\n",
    "    cross_sectional_areas_z = compute_cross_sectional_areas(vertices, axis_index=2)\n",
    "    volume_to_surface_area = volume / surface_area\n",
    "    curvature = calculate_curvature(vertices)\n",
    "    fourier_descriptors = calculate_fourier_descriptors(vertices)\n",
    "    fourier_descriptors_x = compute_fourier_descriptors_cross_sections(vertices, axis_index=0)\n",
    "    fourier_descriptors_y = compute_fourier_descriptors_cross_sections(vertices, axis_index=1)\n",
    "    fourier_descriptors_z = compute_fourier_descriptors_cross_sections(vertices, axis_index=2)\n",
    "    sphericity = calculate_sphericity(volume, surface_area)\n",
    "    roundness = calculate_roundness(vertices)\n",
    "\n",
    "    return np.array([*dimensions, volume, surface_area, *aspect_ratios, elongation, compactness, compactness_aabb, eccentricity, *cross_sectional_areas_x, *cross_sectional_areas_y, *cross_sectional_areas_z, volume_to_surface_area, *variance_ratios, *principal_axes.flatten(), curvature, *fourier_descriptors, *fourier_descriptors_x, *fourier_descriptors_y, *fourier_descriptors_z, sphericity, roundness])\n",
    "\n",
    "def process_stl_files(folder_path):\n",
    "    features = []\n",
    "    file_info = []\n",
    "    vertices_dict = load_stl(folder_path)\n",
    "\n",
    "    for file_path, vertices in vertices_dict.items():\n",
    "        print(f\"Processing {file_path}...\")\n",
    "        model_features = extract_features(vertices)\n",
    "        features.append(model_features)\n",
    "        file_name = os.path.basename(file_path)\n",
    "        folder_name = os.path.dirname(file_path)\n",
    "        file_info.append((file_name, folder_name))\n",
    "\n",
    "    return np.array(features), file_info\n",
    "\n",
    "def export_to_csv(features, file_info, folder_path):\n",
    "    column_names = [\n",
    "        'Length', 'Width', 'Height', 'Volume', 'Surface Area',\n",
    "        'Aspect Ratio 1', 'Aspect Ratio 2', 'Aspect Ratio 3',\n",
    "        'Elongation', 'Compactness', 'Compactness_aabb', 'Eccentricity',\n",
    "        'Cross-Sectional Area X1', 'Cross-Sectional Area X2', 'Cross-Sectional Area X3', 'Cross-Sectional Area X4', 'Cross-Sectional Area X5', 'Cross-Sectional Area X6', 'Cross-Sectional Area X7', 'Cross-Sectional Area X8', 'Cross-Sectional Area X9',  # Assuming 5 sections as per n_sections\n",
    "        'Cross-Sectional Area Y1', 'Cross-Sectional Area Y2', 'Cross-Sectional Area Y3', 'Cross-Sectional Area Y4', 'Cross-Sectional Area Y5', 'Cross-Sectional Area Y6', 'Cross-Sectional Area Y7', 'Cross-Sectional Area Y8', 'Cross-Sectional Area Y9',  # Assuming 5 sections as per n_sections\n",
    "        'Cross-Sectional Area Z1', 'Cross-Sectional Area Z2', 'Cross-Sectional Area Z3', 'Cross-Sectional Area Z4', 'Cross-Sectional Area Z5', 'Cross-Sectional Area Z6', 'Cross-Sectional Area Z7', 'Cross-Sectional Area Z8', 'Cross-Sectional Area Z9',  # Assuming 5 sections as per n_sections\n",
    "        'Volume to Surface Area Ratio', 'PCA Variance Ratio 1', 'PCA Variance Ratio 2', 'PCA Variance Ratio 3',\n",
    "        'PCA Component 1-1', 'PCA Component 1-2', 'PCA Component 1-3',\n",
    "        'PCA Component 2-1', 'PCA Component 2-2', 'PCA Component 2-3',\n",
    "        'PCA Component 3-1', 'PCA Component 3-2', 'PCA Component 3-3',\n",
    "        'Curvature', 'Sphericity', 'Roundness'\n",
    "    ]\n",
    "\n",
    "    # Adding column names for Fourier descriptors from XY, XZ, YZ projections\n",
    "    for plane in ['XY', 'XZ', 'YZ']:\n",
    "        for descriptor in range(1, 11):  # Assuming 10 descriptors\n",
    "            column_names.append(f'Fourier Descriptor {plane}-{descriptor}')\n",
    "\n",
    "    # Adding column names for Fourier descriptors for each cross section\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        for section in range(1, 10):  # Assuming 9 sections as per n_sections\n",
    "            for descriptor in range(1, 11):  # Assuming 10 descriptors\n",
    "                column_names.append(f'Cross-Section Fourier Descriptor {axis}{section}-{descriptor}')\n",
    "\n",
    "    df = pd.DataFrame(features, columns=column_names)\n",
    "    df['File Name'] = [info[0] for info in file_info]\n",
    "    df['Folder Path'] = [info[1] for info in file_info]\n",
    "    df['Class'] = ''  # Add an empty 'Class' column for the user to fill in later\n",
    "\n",
    "    output_path = os.path.join(os.path.dirname(folder_path), 'measures_file.csv')  # Replace with the your own file name\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Data exported to {output_path}\")\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = 'C:/3D_models/oriented_SH'  # Replace with the folder path in which the stl files are stored (it will also extract the files from subfolders)\n",
    "features, file_info = process_stl_files(folder_path)\n",
    "\n",
    "# Export the measurements to CSV\n",
    "export_to_csv(features, file_info, folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd90a26f-93bc-4de8-bde5-a2cdaee0cf54",
   "metadata": {},
   "source": [
    "### Normalise the values of the cvs\n",
    "**Do not execute** unless you are going to use a linear classifier. Even when using a linear classifier such as LDA or SVM, it is recommended to test the performance of the classification agaisnt non-normalised values. <br> Specify the path of the required input file (line 30) with the measures extrated before. <br> Specify the path of the output normalised measures to use later (line 31)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480baaf-dd7d-4f4e-9d4b-c083928cba4f",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_and_round_csv(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Reads a CSV file, normalizes numerical columns to a range of 0 to 1 (excluding all-NaN columns),\n",
    "    rounds them to 4 decimal places, and saves the result to a new CSV file.\n",
    "    \"\"\"\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Select numerical columns and exclude columns with all NaN values\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cols_to_normalize = [col for col in numerical_cols if not df[col].isna().all()]\n",
    "\n",
    "    # Normalize the selected columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df[cols_to_normalize] = scaler.fit_transform(df[cols_to_normalize])\n",
    "\n",
    "    # Round to 4 decimal places\n",
    "    df[cols_to_normalize] = df[cols_to_normalize].round(4)\n",
    "\n",
    "    # Save to new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Normalized and rounded CSV saved to {output_file}\")\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    # Manually set the input and output file paths\n",
    "    input_file_path = 'C:/3D_models/oriented_SH/measures_file.csv'  # Replace with your input file path\n",
    "    output_file_path = 'C:/3D_models/oriented_SH/measures_file_norm.csv'  # Replace with your output file path\n",
    "\n",
    "    normalize_and_round_csv(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea5483e-9546-4f10-865c-96cd588ed4d0",
   "metadata": {},
   "source": [
    "### Get information about the structure of your CSV\n",
    "This will provide some info about the measures extracted that can be useful to evaluate them at a later stage. <br> Provide the path to the csv file with measures you want analyse (line 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a20091-fac0-4a62-ad09-9be74231b0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'C:/3D_models/oriented_SH/measures_file.csv'  # Update this with the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Get some basic stats on the file\n",
    "print(\"Basic File Stats:\")\n",
    "print(df.info())  # This will include the total number of columns\n",
    "print()\n",
    "\n",
    "# For each column, display the first and last 5 rows of unique values\n",
    "for column in df.columns:\n",
    "    unique_values = pd.unique(df[column])\n",
    "    print(f'Column: {column}')\n",
    "    # Check if there are more than 10 unique values to display them as requested\n",
    "    if len(unique_values) > 10:\n",
    "        print(f'Unique values (first 20): {unique_values[:20]}')\n",
    "        print(f'Unique values (last 20): {unique_values[-20:]}')\n",
    "    else:  # If 10 or fewer unique values, just print them all\n",
    "        print(f'Unique values: {unique_values}')\n",
    "    print('-----------------------------------------------------')\n",
    "\n",
    "# Additional statistical summary for numerical columns\n",
    "print(\"Statistical Summary for Numerical Columns:\")\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ec47f-4f5e-4525-8da2-fda66347796b",
   "metadata": {},
   "source": [
    "### Select the column and values within it to create the class values in the csv file 'class' column\n",
    "Use the information obtained above (towards the end of the report) to classify each unique path into a class for classification. <br> This cell uses the input specified in the previous cell. Please, select an output file in line 33. <br>The use of folder paths here is based on the assumption that the models were distributed in different folders (see notes to the first cell), each with one specific characteristic that defines the class, e.g.: 'Dundee\\\\6ROW Bere\\\\6ROW BERE Orkney'. If you have used another way to mark the class, please, specify it in the code below. <br> Please, note that the report only shows the first and last 20 unique paths if there are more paths you will have to include them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b404445-c742-44b9-8538-bdb289711d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to update the 'class' column based on selected unique values from a specific column\n",
    "def update_class_column(df, column_name, class_mapping):\n",
    "    '''\n",
    "    Update the 'class' column in the DataFrame based on selected unique values from a specific column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame to be updated.\n",
    "    column_name (str): The name of the column whose values will be used to populate the 'class' column.\n",
    "    class_mapping (dict): A dictionary mapping the unique values of the selected column to the class names.\n",
    "    '''\n",
    "    df['Class'] = df[column_name].map(class_mapping)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "column_to_use = 'Folder Path'\n",
    "mapping = {'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\2ROW British': 'Brit',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\2ROW Scottish': 'Scot',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\6ROW Bere\\\\6ROW BERE Orkney': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\6ROW Bere\\\\6ROW BERE Unknown': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\6ROW Bere\\\\6ROW BERE Western Isles': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\6ROW Faro': 'Faro',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Dundee\\\\6ROW Scandinavian': 'Scand',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\2ROW British': 'Brit',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\2ROW Scottish': 'Scot',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\6ROW Bere\\\\6ROW BERE ORKNEY': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\6ROW Bere\\\\6ROW BERE Unknown': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\6ROW Bere\\\\6ROW BERE Western Isles': 'Bere',\n",
    "           'C:/ProofsML/OrientedMatched_40K_L50\\\\Orkney\\\\6ROW Scandinavian': 'Scand',}\n",
    "update_class_column(df, column_to_use, mapping)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_file_path = 'C:/3D_models/oriented_SH/measures_file.csv'  # Update this with the actual output file path\n",
    "df.to_csv(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09192e08-f031-4d94-ab6f-e5ff521a5598",
   "metadata": {},
   "source": [
    "### PCA: for lineal classifiers\n",
    "\n",
    "**Do not execute** unless you are going to use a linear classifier. Even when using a linear classifier such as LDA or SVM, it is recommended to test the performance of the classification against standard values. <br> Specify the path of the required input file (line 7) with the measures extrated before. <br> Specify the number of principal components (line 19 and 23). <br> Specify the path of the output normalised measures to use later (line 29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d184177d-2b43-4edb-b557-16409fcb202c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'C:/3D_models/oriented_SH/measures_file.csv'  # Replace with your file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Isolating the numerical and non-numerical columns\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "non_numerical_data = data.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Standardizing the numerical data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Applying PCA\n",
    "pca = PCA(n_components=30)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Converting the principal components into a DataFrame\n",
    "principal_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(30)])\n",
    "\n",
    "# Concatenating the non-numerical data with the principal components\n",
    "final_df = pd.concat([non_numerical_data, principal_df], axis=1)\n",
    "\n",
    "# Save the combined data to a new CSV file\n",
    "final_df.to_csv('C:/3D_models/oriented_SH/measures_file_PCA30.csv', index=False)\n",
    "\n",
    "# Optional: Print statements\n",
    "# To view the explained variance ratio\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# To view the first few rows of the transformed data\n",
    "print(final_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da492d0a-93c0-46a5-a35c-88d69c10027f",
   "metadata": {},
   "source": [
    "### Select, execute, and validate the classifier\n",
    "Select the csv file with the extracted measures and the classes defined previously (line 16) <br>\n",
    "Select the percentage of the total data available for testing purposes (from 0 to 1, line 23). All other data will be used to train the classifier. <br>\n",
    "Select the classifier to use (line 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f505a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, LeaveOneOut\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('C:/3D_models/oriented_SH/measures_file.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X = df.drop(['File Name', 'Folder Path', 'Class'], axis=1).values\n",
    "y = LabelEncoder().fit_transform(df['Class'].values)  # Convert classes to integers\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Specify the classifier\n",
    "classifier_name = 'gbm'  # Options: 'rf', 'gbm', 'svm', 'knn', 'lda', 'fcnn'\n",
    "\n",
    "if classifier_name == 'fcnn':\n",
    "    # FCNN setup\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))  # Output layer\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "    print(f\"Classification Report for FCNN:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(f\"Confusion Matrix for FCNN:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "else:\n",
    "    # Traditional classifiers setup\n",
    "    classifiers = {\n",
    "        'rf': RandomForestClassifier(n_estimators=675, random_state=42),\n",
    "        'gbm': GradientBoostingClassifier(random_state=42),\n",
    "        'svm': SVC(random_state=42),\n",
    "        'knn': KNeighborsClassifier(),\n",
    "        'lda': LinearDiscriminantAnalysis()\n",
    "    }\n",
    "    classifier = classifiers[classifier_name]\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "    cv_scores = cross_val_score(classifier, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and Evaluation\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(f\"Classification Report for {classifier_name}:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "    print(f\"Confusion Matrix for {classifier_name}:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Visualize the results of cross-validation\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='--', color='b')\n",
    "    plt.title(f'Cross-Validation Accuracy Scores for {classifier_name}')\n",
    "    plt.xlabel('Fold Number')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(range(1, len(cv_scores) + 1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6a042a",
   "metadata": {},
   "source": [
    "# Alternative LOO Cros-validation\n",
    "This cell copies the same process than the previous but using a leave-one-out cross validation method (more computationally costly but more accurate) to test the performance of the classification. It requires the same inputs: <br>\n",
    "Select the csv file with the extracted measures and the classes defined previously (line 12) <br>\n",
    "Select the classifier to use (line 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b9b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv('C:/3D_models/oriented_SH/measures_file.csv')\n",
    "\n",
    "# Preprocess data\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['Class'].values)  # Convert classes to integers\n",
    "X = df.drop(['File Name', 'Folder Path', 'Class'], axis=1).values\n",
    "\n",
    "# Specify the classifier\n",
    "classifier_name = 'gbm'  # Options: 'rf', 'gbm', 'svm', 'knn', 'lda'\n",
    "\n",
    "# Setup classifiers\n",
    "classifiers = {\n",
    "    'rf': RandomForestClassifier(n_estimators=150, random_state=42),\n",
    "    'gbm': GradientBoostingClassifier(random_state=42),\n",
    "    'svm': SVC(random_state=42),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'lda': LinearDiscriminantAnalysis()\n",
    "}\n",
    "\n",
    "classifier = classifiers[classifier_name]\n",
    "\n",
    "# Initialize LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Use cross_val_predict to make predictions for each leave-one-out split\n",
    "y_pred = cross_val_predict(classifier, X, y, cv=loo, n_jobs=-1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "print(f\"Overall LOOCV Confusion Matrix for {classifier_name}:\\n{conf_matrix}\")\n",
    "\n",
    "# Compute classification report\n",
    "class_report = classification_report(y, y_pred, digits=4, zero_division=0)\n",
    "print(f\"Classification Report for {classifier_name}:\\n{class_report}\")\n",
    "\n",
    "# Compute per-class accuracies\n",
    "class_names = le.classes_\n",
    "class_accuracies = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "for idx, acc in enumerate(class_accuracies):\n",
    "    print(f\"Accuracy for class '{class_names[idx]}': {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac727c",
   "metadata": {},
   "source": [
    "## Find the best number of trees\n",
    "\n",
    "This cell provides a method to estimate the optimal number of trees to employ in a Random Forest classification <br> As in the previous cells, specify the path to the csv file containing the measurements and classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "# Make sure to replace the path with the correct one for your dataset\n",
    "df = pd.read_csv('C:/3D_models/oriented_SH/measures_file.csv')\n",
    "\n",
    "# Preprocess data\n",
    "X = df.drop(['File Name', 'Folder Path', 'Class'], axis=1).values\n",
    "y = LabelEncoder().fit_transform(df['Class'].values)  # Convert classes to integers\n",
    "\n",
    "# Define the range of number of trees to test\n",
    "min_trees = 50\n",
    "max_trees = 1050\n",
    "step_size = 50\n",
    "\n",
    "# Initialize results storage\n",
    "results = {'Number of Trees': [], 'Accuracy': []}\n",
    "\n",
    "# Setup LeaveOneOut for cross-validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "for n_trees in range(min_trees, max_trees + 1, step_size):\n",
    "    classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "    \n",
    "    # Use cross_val_predict to make predictions for each LOOCV split\n",
    "    y_pred = cross_val_predict(classifier, X, y, cv=loo, n_jobs=-1)\n",
    "    \n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results['Number of Trees'].append(n_trees)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    print(f\"Done for {n_trees} trees: Accuracy = {accuracy}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results['Number of Trees'], results['Accuracy'], marker='o')\n",
    "plt.title('LOOCV Accuracy vs. Number of Trees in Random Forest')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
